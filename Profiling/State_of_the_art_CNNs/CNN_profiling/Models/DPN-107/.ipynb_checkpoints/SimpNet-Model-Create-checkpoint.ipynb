{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Contains the definition for the PNASNet classification networks.\n",
    "\n",
    "Paper: https://arxiv.org/abs/1712.00559\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from cell import PNASCell\n",
    "\n",
    "arg_scope = tf.contrib.framework.arg_scope\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "# Notes for training large PNASNet model on ImageNet\n",
    "# -------------------------------------\n",
    "# batch size (per replica): 16\n",
    "# learning rate: 0.015 * 100\n",
    "# learning rate decay factor: 0.97\n",
    "# num epochs per decay: 2.4\n",
    "# sync sgd with 100 replicas\n",
    "# auxiliary head loss weighting: 0.4\n",
    "# label smoothing: 0.1\n",
    "# clip global norm of all gradients by 10\n",
    "def large_imagenet_config():\n",
    "  \"\"\"Large ImageNet configuration based on PNASNet-5.\"\"\"\n",
    "  return tf.contrib.training.HParams(\n",
    "      stem_multiplier=3.0,\n",
    "      dense_dropout_keep_prob=0.5,\n",
    "      num_cells=12,\n",
    "      filter_scaling_rate=2.0,\n",
    "      num_conv_filters=216,\n",
    "      drop_path_keep_prob=0.6,\n",
    "      use_aux_head=1,\n",
    "      num_reduction_layers=2,\n",
    "      total_training_steps=250000,\n",
    "      num_stem_cells=2,\n",
    "  )\n",
    "\n",
    "\n",
    "def calc_reduction_layers(num_cells, num_reduction_layers):\n",
    "    reduction_layers = []\n",
    "    for pool_num in range(1, num_reduction_layers + 1):\n",
    "        layer_num = (float(pool_num) / (num_reduction_layers + 1)) * num_cells\n",
    "        layer_num = int(layer_num)\n",
    "        reduction_layers.append(layer_num)\n",
    "    return reduction_layers\n",
    "\n",
    "\n",
    "def pnasnet_large_arg_scope(weight_decay=4e-5,\n",
    "                            batch_norm_decay=0.9997,\n",
    "                            batch_norm_epsilon=1e-3):\n",
    "    batch_norm_params = {\n",
    "          'decay': batch_norm_decay, # decay for the moving averages\n",
    "          'epsilon': batch_norm_epsilon, # epsilon to prevent 0s in variance\n",
    "          'scale': True,\n",
    "          'fused': True,\n",
    "      }\n",
    "    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n",
    "    weights_initializer = tf.contrib.layers.variance_scaling_initializer(mode='FAN_OUT')\n",
    "    with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],\n",
    "                 weights_regularizer=weights_regularizer,\n",
    "                 weights_initializer=weights_initializer):\n",
    "        with arg_scope([slim.fully_connected],\n",
    "                   activation_fn=None, scope='FC'):\n",
    "            with arg_scope([slim.conv2d, slim.separable_conv2d],\n",
    "                     activation_fn=None, biases_initializer=None):\n",
    "                with arg_scope([slim.batch_norm], **batch_norm_params) as sc:\n",
    "                    return sc\n",
    "\n",
    "\n",
    "def _imagenet_stem(inputs, hparams, stem_cell):\n",
    "    num_stem_filters = int(32 * hparams.stem_multiplier)\n",
    "    net = slim.conv2d(inputs, num_stem_filters, [3, 3], stride=2, scope='conv0', padding='VALID')\n",
    "    net = slim.batch_norm(net, scope='conv0_bn')\n",
    "\n",
    "  # Run the reduction cells\n",
    "    cell_outputs = [None, net]\n",
    "    filter_scaling = 1.0 / (hparams.filter_scaling_rate**hparams.num_stem_cells)\n",
    "    for cell_num in range(hparams.num_stem_cells):\n",
    "        net = stem_cell(\n",
    "            net,\n",
    "            scope='cell_stem_{}'.format(cell_num),\n",
    "            filter_scaling=filter_scaling,\n",
    "            stride=2,\n",
    "            prev_layer=cell_outputs[-2],\n",
    "            cell_num=cell_num)\n",
    "        cell_outputs.append(net)\n",
    "        filter_scaling *= hparams.filter_scaling_rate\n",
    "    return net, cell_outputs\n",
    "\n",
    "\n",
    "def _build_aux_head(net, end_points, num_classes, hparams, scope):\n",
    "    with tf.variable_scope(scope):\n",
    "        aux_logits = tf.identity(net)\n",
    "        with tf.variable_scope('aux_logits'):\n",
    "            aux_logits = slim.avg_pool2d(\n",
    "            aux_logits, [5, 5], stride=3, padding='VALID')\n",
    "            aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope='proj')\n",
    "            aux_logits = slim.batch_norm(aux_logits, scope='aux_bn0')\n",
    "            aux_logits = tf.nn.relu(aux_logits)\n",
    "            # Shape of feature map before the final layer.\n",
    "            shape = aux_logits.shape\n",
    "            shape = shape[1:3]\n",
    "            aux_logits = slim.conv2d(aux_logits, 768, shape, padding='VALID')\n",
    "            aux_logits = slim.batch_norm(aux_logits, scope='aux_bn1')\n",
    "            aux_logits = tf.nn.relu(aux_logits)\n",
    "            aux_logits = tf.contrib.layers.flatten(aux_logits)\n",
    "            aux_logits = slim.fully_connected(aux_logits, num_classes)\n",
    "            end_points['AuxLogits'] = aux_logits\n",
    "\n",
    "\n",
    "def build_pnasnet_large(images,\n",
    "                        num_classes,\n",
    "                        is_training=True,\n",
    "                        final_endpoint=None,\n",
    "                        config=None):\n",
    "    hparams = large_imagenet_config()\n",
    "    if not is_training:\n",
    "        hparams.set_hparam('drop_path_keep_prob', 1.0)\n",
    "\n",
    "    total_num_cells = hparams.num_cells + hparams.num_stem_cells\n",
    "    cell = PNASCell(hparams.num_conv_filters, hparams.drop_path_keep_prob,\n",
    "                  total_num_cells, hparams.total_training_steps)\n",
    "    with arg_scope([slim.dropout, slim.batch_norm], is_training=is_training):\n",
    "        end_points = {}\n",
    "        def add_and_check_endpoint(endpoint_name, net):\n",
    "            end_points[endpoint_name] = net\n",
    "            return final_endpoint and (endpoint_name == final_endpoint)\n",
    "\n",
    "    # Find where to place the reduction cells or stride normal cells\n",
    "    reduction_indices = calc_reduction_layers(\n",
    "        hparams.num_cells, hparams.num_reduction_layers)\n",
    "\n",
    "    net, cell_outputs = _imagenet_stem(images, hparams, cell)\n",
    "    if add_and_check_endpoint('Stem', net):\n",
    "        return net, end_points\n",
    "\n",
    "    # Setup for building in the auxiliary head.\n",
    "    aux_head_cell_idxes = []\n",
    "    if len(reduction_indices) >= 2:\n",
    "        aux_head_cell_idxes.append(reduction_indices[1] - 1)\n",
    "\n",
    "    # Run the cells\n",
    "    filter_scaling = 1.0\n",
    "    for cell_num in range(hparams.num_cells):\n",
    "        is_reduction = cell_num in reduction_indices\n",
    "        stride = 2 if is_reduction else 1\n",
    "        if is_reduction: filter_scaling *= hparams.filter_scaling_rate\n",
    "        net = cell(\n",
    "          net,\n",
    "          scope='cell_{}'.format(cell_num),\n",
    "          filter_scaling=filter_scaling,\n",
    "          stride=stride,\n",
    "          prev_layer=cell_outputs[-2],\n",
    "          cell_num=hparams.num_stem_cells + cell_num)\n",
    "        if add_and_check_endpoint('Cell_{}'.format(cell_num), net):\n",
    "            return net, end_points\n",
    "        cell_outputs.append(net)\n",
    "\n",
    "        if (hparams.use_aux_head and cell_num in aux_head_cell_idxes and\n",
    "          num_classes and is_training):\n",
    "            aux_net = tf.nn.relu(net)\n",
    "            _build_aux_head(aux_net, end_points, num_classes, hparams,\n",
    "                        scope='aux_{}'.format(cell_num))\n",
    "        # Final softmax layer\n",
    "        with tf.variable_scope('final_layer'):\n",
    "            net = tf.nn.relu(net)\n",
    "            net = tf.reduce_mean(net, [1, 2])\n",
    "            if add_and_check_endpoint('global_pool', net) or not num_classes:\n",
    "                return net, end_points\n",
    "        \n",
    "        net = slim.dropout(net, hparams.dense_dropout_keep_prob, scope='dropout')\n",
    "        logits = slim.fully_connected(net, num_classes)\n",
    "        if add_and_check_endpoint('Logits', logits):\n",
    "            return net, end_points\n",
    "        \n",
    "        predictions = tf.nn.softmax(logits, name='predictions')\n",
    "        if add_and_check_endpoint('Predictions', predictions):\n",
    "            return net, end_points\n",
    "    return logits, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# Save model architecture to json file\n",
    "model_json = model.to_json()\n",
    "with open(\"./Saved-Model/MNASNet1.0.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the h5 file to path specified.\n",
    "model.save(\"./Saved-Model/MNASNet1.0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Output dense_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to dense_1.\n",
      "['input_2'] ['dense_1/Softmax']\n",
      "WARNING:tensorflow:From <ipython-input-9-b6c4a47b9122>:12: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.remove_training_nodes`\n",
      "WARNING:tensorflow:From <ipython-input-9-b6c4a47b9122>:13: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/rofaida/python-envs/env2/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 262 variables.\n",
      "INFO:tensorflow:Converted 262 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_io\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Clear any previous session.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "save_pb_dir = './Saved-Model'\n",
    "model_fname = './Saved-Model/MNASNet1.0.h5'\n",
    "def freeze_graph(graph, session, output, save_pb_dir='.', save_pb_name='MNASNet1.0-tf-graph.pb', save_pb_as_text=False):\n",
    "    with graph.as_default():\n",
    "        graphdef_inf = tf.graph_util.remove_training_nodes(graph.as_graph_def())\n",
    "        graphdef_frozen = tf.graph_util.convert_variables_to_constants(session, graphdef_inf, output)\n",
    "        graph_io.write_graph(graphdef_frozen, save_pb_dir, save_pb_name, as_text=save_pb_as_text)\n",
    "        return graphdef_frozen\n",
    "    \n",
    "# This line must be executed before loading Keras model.\n",
    "tf.keras.backend.set_learning_phase(0) \n",
    "\n",
    "model = load_model(model_fname)\n",
    "\n",
    "session = tf.keras.backend.get_session()\n",
    "\n",
    "input_names = [t.op.name for t in model.inputs]\n",
    "output_names = [t.op.name for t in model.outputs]\n",
    "\n",
    "# Prints input and output nodes names, take notes of them.\n",
    "print(input_names, output_names)\n",
    "\n",
    "frozen_graph = freeze_graph(session.graph, session, [out.op.name for out in model.outputs], save_pb_dir=save_pb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
